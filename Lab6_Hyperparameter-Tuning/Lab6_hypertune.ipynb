{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 6 Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader,Subset,Dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from skimage.util import random_noise\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "seed = 4912\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Complete the class `CustomImageDataset()` that `__getitem__` return ***noisy blury*** image and ***ground truth*** image.\n",
    "Please ensure that the final image is in RGBscale and has a size of 128x128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_paths,gauss_noise=False,gauss_blur=None,resize=128,p=0.5):\n",
    "        self.p = p\n",
    "        self.resize = resize\n",
    "        self.gauss_noise = gauss_noise\n",
    "        self.gauss_blur = gauss_blur\n",
    "        self.center_crop = center_crop\n",
    "        self.image_paths = image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image_paths = self.image_paths[idx]\n",
    "        pass\n",
    "\n",
    "        return image, gt_image\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "def imshow_grid(images):\n",
    "    pass\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "data_dir = None\n",
    "dataset = CustomImageDataset(None\n",
    "                            )\n",
    "dataloader = DataLoader(dataset, batch_size=None)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "batch,gt_img = next(iter(dataloader)) \n",
    "\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Autoencoder model\n",
    "You can design your own Autoencoder model with a customizable number of downsampling and upsampling blocks by passing a list of channel numbers for each layer based on the provided code below. However, please maintain the concept of 'Autoencoder'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "class DownSamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(DownSamplingBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class UpSamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(UpSamplingBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, channels=[64, 128, 256], input_channels=3, output_channels=3):\n",
    "        super().__init__()\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        return x\n",
    "\n",
    "### END CODE HERE ###\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoencoder\n",
    "Complete the `train()` function in the cell below. This function should evaluate the model at every epoch, log the ***training loss, test loss,test PSNR, test SSIM***. Additionally, it should save the model at the last epoch.\n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"orange\">\n",
    "<b>Expected output</b>\n",
    "</font>\n",
    "</summary>\n",
    "\n",
    "- The log should resemble this, but not be identical\n",
    "\n",
    "```\n",
    "ðŸ¤–Training on cuda\n",
    "ðŸš€Training Epoch [1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1313/1313 [01:45<00:00, 12.41batch/s, loss=0.0102] \n",
    "ðŸ“„Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 563/563 [01:10<00:00,  7.95batch/s, loss=0.0106, psnr=16.7, ssim=0.348] \n",
    "Summary :\n",
    "\tTrain\tavg_loss: 0.017262999383663165\n",
    "\tTest\tavg_loss: 0.010476540363861867 \n",
    "                PSNR : 16.839487147468034 \n",
    "                SSIM : 0.36090552368883694\n",
    "...\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "Resource : [PyTorch Training loop](<https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#:~:text=%3D0.9)-,The%20Training%20Loop,-Below%2C%20we%20have>), [PSNR & SSIM](https://ieeexplore.ieee.org/document/5596999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "def train(model,opt,loss_fn,train_loader,test_loader,epochs=10,checkpoint_path=None,device='cpu'):\n",
    "    print(\"ðŸ¤–Training on\", device)\n",
    "    model = model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_bar = tqdm(train_loader,desc=f'ðŸš€Training Epoch [{epoch+1}/{epochs}]',unit='batch')\n",
    "        for images, gt in train_bar:\n",
    "            pass\n",
    "            \n",
    "        model.eval()\n",
    "        test_bar = tqdm(test_loader,desc='ðŸ“„Testing',unit='batch')\n",
    "        for images, gt in test_bar:\n",
    "            pass\n",
    "                \n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train your model with 2 epochs to verify that your train() function works properly. After that, we'll move on to the Hyperparameter Grid Search in the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "data_dir = None\n",
    "\n",
    "files = os.listdir(data_dir)\n",
    "files = [os.path.join(data_dir, file) for file in files]\n",
    "\n",
    "\n",
    "train_files, test_files = train_test_split(None)\n",
    "\n",
    "\n",
    "train_dataset = CustomImageDataset(None)\n",
    "test_dataset = CustomImageDataset(None)\n",
    "trainloader = DataLoader(None)\n",
    "testloader = DataLoader(None)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "model = Autoencoder()\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "train(None)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyperparameter Grid Search with Raytune**\n",
    "\n",
    "*If you have access to APEX, I would recommend converting this part into a Python file and submitting the job to run on APEX using SBATCH. This process may take a considerable amount of time.*\n",
    "\n",
    "You can import additional Ray Tune tools as you want, such as schedulers, search algorithms, etc. Further information on the usage of Ray Tune can be found [here](https://docs.ray.io/en/latest/tune/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the `train_raytune()` function below, following the [quick start guide](https://docs.ray.io/en/latest/tune/index.html). This function will be passed to the `tune.Tuner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "def train_raytune(config):\n",
    "\n",
    "\n",
    "\n",
    "    trainloader = None\n",
    "    testloader = None\n",
    "    \n",
    "    model = None\n",
    "\n",
    "\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        pass\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        pass\n",
    "\n",
    "\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        model.train()\n",
    "        \n",
    "        avg_train_loss = None\n",
    "        avg_test_loss = None\n",
    "        \n",
    "        total_psnr = None\n",
    "        total_ssim = None\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "        session.report({\n",
    "            \"train_loss\": None,\n",
    "            \"val_loss\": None,\n",
    "            \"val_psnr\": None,\n",
    "            \"val_ssim\": None,\n",
    "        })\n",
    "        \n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Ray, define the search space, and resources.\n",
    "\n",
    "Resource : \n",
    "- [A Guide To Parallelism and Resources for Ray Tune](https://docs.ray.io/en/latest/tune/tutorials/tune-resources.html#:~:text=A%20Guide%20To%20Parallelism%20and%20Resources%20for%20Ray%20Tune) \n",
    "- [Working with Tune Search Spaces](https://docs.ray.io/en/latest/tune/tutorials/tune-search-spaces.html#tune-search-space-tutorial:~:text=Working%20with%20Tune%20Search%20Spaces)\n",
    "- [How to configure logging in Tune?](https://docs.ray.io/en/latest/tune/tutorials/tune-output.html) \n",
    "- [Tune Trial Schedulers (`tune.schedulers`)](https://docs.ray.io/en/latest/tune/api/schedulers.html#tune-scheduler-pbt:~:text=Tune%20Trial...-,Tune%20Trial%20Schedulers%20(tune.schedulers),-%23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search Space:**\n",
    "- `architecture`:<br>\n",
    "    Feature map dimensions for convolutional layers<br>\n",
    "    - `[32, 64, 128]`: 3 downsampling layers with feature maps increasing from 32 to 128.\n",
    "    - `[64, 128, 256]`: 3 downsampling layers with feature maps starting from 64 to 256.\n",
    "    - `[64, 128, 256, 512]`: 4 downsampling layers with more depth, starting from 64 and ending at 512.\n",
    "- `learning rates (lr)`:\n",
    "    - [1e-3, 8e-4, 1e-4, 1e-2]: Test a wide range of learning rates to evaluate model performance, from 1e-3 (typical) to a more aggressive 1e-2 or conservative 1e-4.\n",
    "- `batch size`:\n",
    "    - [16, 32]: Explore smaller batch sizes to evaluate their impact on gradient estimation and memory usage.\n",
    "- `number of epochs`:\n",
    "    - `[10, 50, 100]`: Allow short and long training sessions, from quick evaluations (10 epochs) to more extensive training (100 epochs).\n",
    "- `optimizers (opts)`:\n",
    "    - `[\"Adam\", \"SGD\"]`: Compare two popular optimization algorithms: Adam for adaptive learning rates and SGD for momentum-based updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "ray.init(num_gpus=1,)\n",
    "\n",
    "config = {\n",
    "    'None': None,\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    None\n",
    ")\n",
    "result = tuner.fit()\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore the result from path of ray resule directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "path = \"\"\n",
    "restored_tuner = tune.Tuner.restore(path, trainable='train_raytune')\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the report from Grid Search to CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ‰[INFO] Training is done!\")\n",
    "print(\"Best config is:\", result.get_best_result().config)\n",
    "print(\"Best result is:\", result.get_best_result())\n",
    "df = result.get_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Autoencoder models using the best hyperparameter set obtained from the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `FeatureExtractor()` class and `visualize_feature_map()` function to visualize the feature map of ***ALL*** layers. Then, save it as an image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class FeatureMapVisualizer:\n",
    "    def __init__(self, model, layers, save_dir):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - model: The PyTorch model\n",
    "        - layers: A string or list of strings specifying the layer names to visualize\n",
    "        - save_dir: Directory to save the output feature map images\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.layers = layers if isinstance(layers, list) else [layers]\n",
    "        self.activations = {}\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for name, layer in self.model.named_modules():\n",
    "            if name in self.layers:\n",
    "                layer.register_forward_hook(self._hook_fn(name))\n",
    "\n",
    "    def _hook_fn(self, layer_name):\n",
    "        def hook(module, input, output):\n",
    "            print(f'Hooking layer: {layer_name}')\n",
    "            self.activations[layer_name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    def visualize(self, input_paths):\n",
    "        \"\"\"\n",
    "        Pass an input tensor through the model and visualize the activations.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_paths: List of image paths to pass through the model\n",
    "        \"\"\"\n",
    "        \n",
    "        for img_path in input_paths:\n",
    "            self.model(img_path)\n",
    "\n",
    "            for layer_name, activation in self.activations.items():\n",
    "                print(f'Visualizing and saving layer: {layer_name}')\n",
    "                self._save_feature_maps(activation, layer_name)\n",
    "\n",
    "    def _save_feature_maps(self, activation, layer_name):\n",
    "        ### START CODE HERE ###\n",
    "        num_channels = activation.shape[1]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "visualizer = FeatureMapVisualizer(None)\n",
    "visualizer.visualize([None])\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Hyperparameter Random Search with Raytune**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search Space:**\n",
    "\n",
    "- **`architecture`:**  \n",
    "    Define the feature map dimensions for convolutional layers:  \n",
    "    - `[32, 64, 128]`: 3 downsampling layers with feature maps increasing from 32 to 128.\n",
    "    - `[64, 128, 256]`: 3 downsampling layers with feature maps starting from 64 to 256.\n",
    "    - `[64, 128, 256, 512]`: 4 downsampling layers with additional depth, starting from 64 and ending at 512.\n",
    "  \n",
    "- **`learning rates (lr)`**:  \n",
    "    A continuous range of learning rates sampled uniformly between `1e-4` and `1e-2`. This allows exploration of different learning rates from conservative (`1e-4`) to more aggressive (`1e-2`) values.\n",
    "\n",
    "- **`batch size`**:  \n",
    "    Randomly sample an integer batch size between 16 and 32 (inclusive). This allows testing of smaller batch sizes, which can affect gradient estimation and memory usage.\n",
    "\n",
    "- **`number of epochs`**:  \n",
    "    Randomly sample an integer number of epochs between 10 and 100. This allows the model to train for short (e.g., 10 epochs) or extended periods (up to 100 epochs), giving insight into model performance over different training durations.\n",
    "\n",
    "- **`optimizers (opts)`**:  \n",
    "    Randomly select between two optimizers:  \n",
    "    - `\"Adam\"`: An adaptive learning rate optimizer that generally performs well across various tasks.  \n",
    "    - `\"SGD\"`: Stochastic Gradient Descent with momentum, commonly used for large-scale tasks, requiring careful tuning of the learning rate.\n",
    "\n",
    "***NOTE*** Random search with 80 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "ray.shutdown()\n",
    "ray.init(num_gpus=1)\n",
    "\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ‰[INFO] Training is done!\")\n",
    "print(\"Best config is:\", result.get_best_result().config)\n",
    "print(\"Best result is:\", result.get_best_result())\n",
    "df = result.get_dataframe()\n",
    "df.to_csv('', index=False)\n",
    "\n",
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Autoencoder models using the best hyperparameter set obtained from the random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `FeatureExtractor()` class and `visualize_feature_map()` function to visualize the feature map of ***ALL*** layers of the Convolution Feature Extractor part. Then, save it as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "visualizer = FeatureMapVisualizer(None)\n",
    "visualizer.visualize([None])\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. How many combinations of hyperparameter values (trials) were evaluated during the hyperparameter tuning process?\n",
    "2. What are the top 3 best parameters and their corresponding tuning results for the model?\n",
    "3. Analyze and compare the similarities and differences between the top 3 parameters in terms of model architecture, loss, performance, etc.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
